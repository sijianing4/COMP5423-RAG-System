
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
from typing import List, Dict

class BasicGenerator:
    """åŸºç¡€ç”Ÿæˆå™¨ - åŸºäºä½ çš„demoä»£ç """
    
    def __init__(self, model_name: str = "Qwen/Qwen2.5-0.5B-Instruct"):
        """
        åˆå§‹åŒ–ç”Ÿæˆå™¨
        
        Args:
            model_name: æ¨¡å‹åç§°
        """
        self.model_name = model_name
        print(f"ğŸ¤– åŠ è½½æ¨¡å‹: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def generate_answer(self, question: str, retrieved_docs: List[Dict]) -> str:
        """
        ä½¿ç”¨æ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            retrieved_docs: æ£€ç´¢åˆ°çš„æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            str: ç”Ÿæˆçš„ç­”æ¡ˆ
        """
        # æ„å»ºæç¤ºæ¨¡æ¿
        context = "\n".join([f"[æ–‡æ¡£ {i+1}, ID: {doc['id']}]: {doc['content']}"
                            for i, doc in enumerate(retrieved_docs)])

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½é—®ç­”åŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹æä¾›çš„æ–‡æ¡£å†…å®¹ï¼Œå‡†ç¡®å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚åªä½¿ç”¨æ–‡æ¡£ä¸­çš„ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ å†…å®¹ã€‚

ç›¸å…³æ–‡æ¡£ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·åŸºäºä¸Šè¿°æ–‡æ¡£æä¾›å‡†ç¡®ã€ç®€æ´çš„ç­”æ¡ˆã€‚å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®è¯´æ˜"æ ¹æ®æä¾›çš„æ–‡æ¡£ï¼Œæ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜"ã€‚

ç­”æ¡ˆï¼š"""

        # å‡†å¤‡æ¨¡å‹è¾“å…¥
        messages = [{"role": "user", "content": prompt}]
        text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)

        # ç”Ÿæˆå›ç­”
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=300,
                do_sample=True,
                temperature=0.3,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id
            )

        # è§£ç è¾“å‡º
        response = self.tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)
        return response.strip()
    
    def batch_generate(self, questions: List[str], all_retrieved_docs: List[List[Dict]]) -> List[str]:
        """
        æ‰¹é‡ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            questions: é—®é¢˜åˆ—è¡¨
            all_retrieved_docs: æ¯ä¸ªé—®é¢˜å¯¹åº”çš„æ£€ç´¢æ–‡æ¡£åˆ—è¡¨
            
        Returns:
            List[str]: ç­”æ¡ˆåˆ—è¡¨
        """
        answers = []
        for question, retrieved_docs in zip(questions, all_retrieved_docs):
            answer = self.generate_answer(question, retrieved_docs)
            answers.append(answer)
        return answers
